<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="styles/styles.css">
  <title>MACH-IV Clustering</title>
  <body>
    <header>
      <h1>MACH-IV Clustering Exploration</h1>
      <p>Using clustering algorithms to explore patterns in Machiavellianism (MACH-IV) personality data. Created by Jeffrey Hoelzel Jr, Sean Golez, and Luke Bowen.</p>
    </header>

    <nav>
      <ul>
        <li><a href="#dataset">Dataset</a></li>
        <li><a href="#kmeans">K-Means</a></li>
        <li><a href="#gmm">Gaussian Mixture Model</a></li>
        <li><a href="#spectral">Spectral Clustering</a></li>
        <li><a href="#hierarchical">Hierarchical Clustering</a></li>
        <li><a href="#future-work">Future Work</a></li>
      </ul>
    </nav>

    <main>
      <section class="intro-card">
        <h2>Project Overview</h2>
        <p>We worked with a sample of Machiavellian response vectors where each participant answered twenty items on a Likert-scale from one to five. Each method grouped people based on their answer patterns, allowing us to compare how different algorithms carve the data into clusters that might correspond to low, medium, and high levels of Machiavellian traits. The dataset used can be found <a target="_blank" href="https://openpsychometrics.org/_rawdata/">here</a>.</p>
        <div class="summary-box">
          <h3>On this site, you can find:</h3>
          <ul>
            <li>Plots that visualize clusters in a low-dimensional space.</li>
            <li>Short written interpretations for each method used.</li>
            <li>Basic validation metrics such as Silhouette scores and PCA.</li>
          </ul>
        </div>
      </section>

    <!-- Dataset -->
     <section id="dataset" class="algorithm-section">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <h2>Dataset</h2>
        </div>
        <div class="algorithm-body-text-only">
          <div class="text-block">
            <p>
              The MACH-IV dataset consists of responses from <strong>73,489 participants</strong> who completed the MACH-IV personality inventory. Each participant answered <strong>20 items on a Likert scale</strong> ranging from 1 (strongly disagree) to 5 (strongly agree). The dataset captures various dimensions of Machiavellian traits, which are often associated with manipulativeness, cynicism, and a pragmatic approach to interpersonal relationships.
            </p>
            <p>
              Prior to applying clustering algorithms, we performed <strong>standard preprocessing</strong> steps including handling missing values and conducting exploratory data analysis to understand the distribution of responses across items. This ensured that our clustering results would be based on clean and meaningful data.
            </p>
            <p>
              There is also much more information provided in this datasets beyond the 20 questions we focused on for this project. When downloading the dataset, there will also be a <strong>codebook provided</strong> that describes all the additional demographic and psychometric variables collected from participants. You can find the raw dataset <a target="_blank" href="https://openpsychometrics.org/_rawdata/">here</a> under the description <i>"Answers to the Machivallianism Test, a version of the MACH-IV from Christie and Geis (1970)."</i>
            </p>
          </div>
     </section>

    <!-- K Means -->
    <section id="kmeans" class="algorithm-section">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <h2>K-Means Clustering</h2>
          <span class="algorithm-tag">Partition based</span>
        </div>
        <div class="algorithm-body">
          <div class="text-block">
            <p>
              K Means partitions the data into a set number of clusters by assigning each participant to the nearest cluster center in feature space. In our analysis, we chose a value of <i>k</i> that balanced interpretability with inter-cluster compactness.
            </p>
            <ul>
              <li>Replace this paragraph with a brief explanation of how you chose K</li>
              <li>Describe the typical response profile in each cluster</li>
              <li>Mention any clusters that look similar and how that compares to other methods</li>
            </ul>
            <table class="metrics-table">
              <thead>
                <tr>
                  <th>Metric</th>
                  <th>Value</th>
                  <th>Notes</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Silhouette score</td>
                  <td>TODO</td>
                  <td>Higher values indicate more separated clusters</td>
                </tr>
                <tr>
                  <td>Calinski Harabasz index</td>
                  <td>TODO</td>
                  <td>Higher values indicate more distinct clusters</td>
                </tr>
                <tr>
                  <td>Davies Bouldin index</td>
                  <td>TODO</td>
                  <td>Lower values indicate better separation</td>
                </tr>
              </tbody>
            </table>
          </div>
          <div class="plot-block">
            <figure>
              <div class="plot-placeholder">
                Insert your main K-Means cluster plot here
              </div>
              <figcaption>
                Suggested plot PCA or t SNE projection colored by cluster label.
              </figcaption>
            </figure>
            <figure>
              <div class="plot-placeholder">
                Optional second plot for K Means for example within cluster item means
              </div>
              <figcaption>
                Suggested plot average item response profile for each cluster.
              </figcaption>
            </figure>
          </div>
        </div>
      </div>
    </section>

    <!-- Gaussian mixture model -->
    <section id="gmm" class="algorithm-section">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <h2>Gaussian Mixture Model</h2>
          <span class="algorithm-tag">Model based</span>
        </div>
        <div class="algorithm-body">
          <div class="text-block">
            <p>
              A Gaussian mixture model treats clusters as overlapping probability distributions rather than hard partitions. Each participant receives a probability of belonging to each component, which can reveal more gradual changes between low and high trait levels.
            </p>
            <ul>
              <li>Explain how you selected the number of components for the model</li>
              <li>Report any information criteria you used such as BIC or AIC</li>
              <li>Comment on where GMM agrees or disagrees with K Means</li>
            </ul>
            <table class="metrics-table">
              <thead>
                <tr>
                  <th>Metric</th>
                  <th>Value</th>
                  <th>Notes</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>BIC</td>
                  <td>TODO</td>
                  <td>Lower values indicate a better tradeoff between fit and complexity</td>
                </tr>
                <tr>
                  <td>AIC</td>
                  <td>TODO</td>
                  <td>Alternative information criterion for model selection</td>
                </tr>
                <tr>
                  <td>Average max membership</td>
                  <td>TODO</td>
                  <td>How confident the model is in its assignments</td>
                </tr>
              </tbody>
            </table>
          </div>
          <div class="plot-block">
            <figure>
              <div class="plot-placeholder">
                Insert GMM cluster visualization here
              </div>
              <figcaption>
                Suggested plot projection colored by most likely component.
              </figcaption>
            </figure>
            <figure>
              <div class="plot-placeholder">
                Optional membership heatmap or density plot
              </div>
              <figcaption>
                Suggested plot matrix of membership probabilities for a sample of participants.
              </figcaption>
            </figure>
          </div>
        </div>
      </div>
    </section>

    <!-- Spectral clustering -->
    <section id="spectral" class="algorithm-section">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <h2>Spectral Clustering</h2>
          <span class="algorithm-tag">Graph based</span>
        </div>
        <div class="algorithm-body">
          <div class="text-block">
            <p>
              Spectral clustering builds a <strong>similarity graph</strong> between participants and uses <strong>eigenvectors</strong> of the graph Laplacian to embed the data before forming clusters. This method can capture <strong>more complex structures</strong> that simple distance based partitions might miss.
            </p>
            <ul>
              <li>We constructed a <strong>similarity matrix</strong> by building a k-nearest-neighbor graph
              using Euclidean distances with <i>k=5</i></li>. We then symmetrized the graph using an <strong>RBF kernel</strong>.
              <li>The results from spectral clustering differ from algorithms like kmeans or GMM in the sense that
                spectral clustering can capture <strong>non-convex cluster shapes</strong> and is sensitive to the choice of graph construction method.
                In practice, the clusters identified by spectral clustering showed boundaries that were <strong>smoother</strong> and <strong>better aligned</strong>
                with the underlying data manifold.
              </li>
              <li>Due to the reliance on the graph Laplacian, spectral clustering can be much more sensitive to local-connectivity than KMeans.
                This tends to force points into similarly sized partitions, which we saw when running spectral clustering with more than 2 clusters.
              </li>
            </ul>
            <table class="metrics-table">
              <thead>
                <tr>
                  <th>Metric</th>
                  <th>Value</th>
                  <th>Notes</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Silhouette score</td>
                  <td>0.XX</td>
                  <td>Spectral and KMeans both used the same metric of silhouette score for direct comparison.</td>
                </tr>
                <tr>
                  <td>Number of clusters</td>
                  <td>K</td>
                  <td>We evaluated values of K ranging from 2 to 4.</td>
                </tr>
                <tr>
                  <td>Neighbor parameter</td>
                  <td>k</td>
                  <td>We ran this simulation using k=15 for the k-nearest-neighbors hyperparameter.</td>
                </tr>
              </tbody>
            </table>
          </div>
          <div class="plot-block">
            <figure>
              <div class="plot-placeholder">
                <img src="images/spectral/spectral_embedding_k_2.png" alt="Spectral embedding plot showing clusters identified by spectral clustering">
              </div>
              <figcaption>
                Spectral Embedding plot ran with 2 clusters on Monsoon.
              </figcaption>
            </figure>
            <figure>
              <div class="plot-placeholder">
                <img src="images/spectral/spectral_embedding_k_3.png" alt="Spectral embedding plot showing clusters identified by spectral clustering">
              </div>
              <figcaption>
                Spectral Embedding plot ran with 3 clusters on Monsoon. Note the inability to smoothly form the third cluster due to the local connectivity constraints of spectral clustering.
              </figcaption>
            </figure>
          </div>
        </div>
        <p>
          <br>Below are two radar plots showing the <strong>Ten Item Personality Inventory (TIPI) personality responses</strong> for both clusters identified using spectral clustering. Each plot gives further insight into the clusters discovered, and shows the distinct levels of <strong>Machiavellianism</strong> within each group.<br><br>
        </p>
          <div class="plot-block-horizontal">
            <figure>
              <div class="plot-placeholder">
                <img src="images/spectral/Cluster_0.png" alt="Degree distribution plot of the similarity graph used in spectral clustering">
              </div>
              <figcaption>
                Cluster 0 represents low-Machiavellian individuals. These individuals scored higher in traits like emotionally stable, dependable, and disciplined, while scoring lower in traits like careless, uncreative, or easily upset.
              </figcaption>
            </figure>
            <figure>
              <div class="plot-placeholder">
                <img src="images/spectral/Cluster_1.png" alt="Degree distribution plot of the similarity graph used in spectral clustering">
              </div>
              <figcaption>
                Cluster 1 represents high-Machiavellian individuals. These individuals scored higher in traits like extraverted, anxious, and disorganized, while scoring lower in traits like reserved, calm, and sympathetic.
              </figcaption>
            </figure>
          </div>
        </div>
      </div>
    </section>

    <!-- Hierarchical clustering -->
    <section id="hierarchical" class="algorithm-section">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <h2>Hierarchical Clustering</h2>
          <span class="algorithm-tag">Tree based</span>
        </div>
        <div class="algorithm-body">
          <div class="text-block">
            <p>
              Hierarchical clustering builds a tree of merges that shows how clusters form at different distance thresholds. This offers a view of structure, known as a <strong>dendrogram</strong>, at many levels rather than a single partition of the data.
            </p>
            <p>
              In our final analysis, we ran Ward linkage hierarchical clustering on <strong>all 73,489 particpants</strong> using NAU's <strong>Monsoon HPC</strong> to handle the computational load. We then sampled <strong>5,000 random participants</strong> to visualize the dendrograms for each linkage rule.
            </p>
            <ul>
              <li>Our final hierarchical clustering algorithm uses <strong>agglomerative clustering</strong> with the <strong>Ward</strong> linkage rule.</li>
              <li>To choose a cut level, we plotted the dendrogram <i>(as seen on the right)</i> and determined that <strong>2, 3, & 4</strong> clusters were good cluster sizes to try.</li>
              <li>We determined <strong>3 clusters</strong> was the sweet spot for hierarchical clustering based on the <strong>PCA plot</strong> <i>(bottom right)</i>, which aligns with most personality-based studies suggesting that a <strong>spectrum</strong> exists among general personality traits.</li>
              <li>Although, each of the three Silhouette scores are <i>fairly low</i>, suggesting there are not clear clusters among our dataset, we expected this since the data is related to personality-based data which is often <strong>more of a spectrum</strong> rather than clear-cut categories.</li>
            </ul>
            <table class="metrics-table">
              <thead>
                <tr>
                  <th>Feature</th>
                  <th>Choice</th>
                  <th>Notes</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Distance metric</td>
                  <td>Euclidean</td>
                  <td>Ward linkage tries to minimize the increase in total inter-cluster variation by minimizing the sum of squared Euclidean distances.</td>
                </tr>
                <tr>
                  <td>Linkage rule</td>
                  <td>Ward</td>
                  <td>Ward linkage offers the most interpretable hierarchy for psychological traits and thus, became our linkage rule of choice.</td>
                </tr>
                <tr>
                  <td>Number of clusters at cut</td>
                  <td>3</td>
                  <td>We decided 3 clusters made the most sense to interpret this data.</td>
                </tr>
              </tbody>
            </table>
          </div>
          <div class="plot-block">
            <figure>
              <div class="plot-placeholder">
                <img src="images/hierarchical/default_dendrograms.png" alt="Four dendrograms showing hierarchical clustering using the four different linkage rules for n=5000 random participants">
              </div>
              <figcaption>
                Four dendrograms showing hierarchical clustering using the four different linkage rules for n=5000 random participants.
              </figcaption>
            </figure>
            <figure>
              <div class="plot-placeholder plot-wide">
                <img src="images/hierarchical/ward_linkage_pca.png" alt="PCA plot showing hierarchical clustering results using Ward linkage on all 73,489 participants">
              </div>
              <figcaption>
                PCA plot for 2, 3, & 4 clusters showing hierarchical clustering results using Ward linkage on all 73,489 participants. The Silhouette scores for 2, 3, and 4 clusters are 0.174, 0.108, and 0.049 respectively.
              </figcaption>
            </figure>
          </div>
        </div>
        <p>
          <br>Below are three radar plots representing the <strong>Ten Item Personality Inventory (TIPI) personality responses</strong> for each of the three clusters identified using hierarchical clustering with Ward linkage. Each plot illustrates the <strong>distinct personality response characteristics of each cluster</strong>, providing insights into the varying levels of Machiavellian traits among participants.<br><br>
        </p>
        <p>
          An important item to consider is the line between low and high Machiavellianism is not very clear according to our research. It it clear that a more in-depth analysis would be required to full understand the nuances of Machiavellian traits across individuals. However, these plots provide a foundational understanding of how different personality traits cluster together in relation to Machiavellianism.<br><br>
        </p>
        <div class="plot-block-horizontal">
          <figure>
            <div class="plot-placeholder">
              <img src="images/hierarchical/Cluster_2.png" alt="Radar plot showing average item responses for cluster 2 using hierarchical clustering with Ward linkage">
            </div>
            <figcaption>
              Cluster 2 represents <strong>low-Machiavellian</strong> individuals who score high in conscientiousness and emotional stability while remaining warm, calm, and cooperative. Their profile reflects a trusting and disciplined personality with little inclination toward manipulation. This group tends to value honesty, structure, and positive social relationships.
          </figure>
          <figure>
            <div class="plot-placeholder">
              <img src="images/hierarchical/Cluster_3.png" alt="Radar plot showing average item responses for cluster 3 using hierarchical clustering with Ward linkage">
            </div>
            <figcaption>
              Cluster 3 shows a moderate, balanced personality profile with no extreme traits in any direction. These individuals fall in the <strong>middle of the Machiavellian</strong> spectrum and demonstrate flexible, context-dependent behavior. They are neither strongly manipulative nor strongly moralistic.
          </figure>
          <figure>
            <div class="plot-placeholder">
              <img src="images/hierarchical/Cluster_1.png" alt="Radar plot showing average item responses for cluster 1 using hierarchical clustering with Ward linkage">
            </div>
            <figcaption>
              Cluster 1 reflects <strong>high-Machiavellian</strong> tendencies characterized by low warmth, high criticalness, strong emotional control, and strategic thinking. These individuals display traits associated with competitiveness, manipulation, and a pragmatic worldview. Their combination of low agreeableness and high conscientiousness aligns with classic Mach personality patterns.
          </figure>
        </div>
      </div>
    </section>

    <!-- Future work -->
     <section id="future-work" class="algorithm-section">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <h2>Future Work</h2>
        </div>
        <div class="algorithm-body-text-only">
          <div class="text-block">
            We recognize several avenues for future exploration to enhance our understanding of the MACH-IV personality data. Not to mention, the inherent flaws in our current implementation that could be improved upon, namely the use of clustering algorithms on personality-based data. Below are a few ideas for future work:
            <ol>
              <li><strong>Train More Expressive Models</strong> - Our current clustering methods rely on distance-based or linear separations. A future direction could be to train <strong>nonlinear models</strong> like neural networks, variational autoencoders, or deep mixture models that may be able to capture more complex and subtle psychological patterns that clustering algorithms simply cannot.</li>
              <li><strong>Use Embeddings Instead of Raw Likert Vectors</strong> - We could represent participants using <strong>learned representations</strong> (possibly from an autoencoder). This would allow for models to learn richer relationships between items, reduce noise, and improve cluster separation.</li>
              <li><strong>Use Supervised Learning to Build a Predictive Model</strong> - The inherent flaw with unsupervided machine learning is you must make assumptions about the data beforehand and also rely solely on whatever algorithm chosen to actually find patterns. A more robust approach could be to collect <strong>labeled data</strong> where participants are pre-categorized into specific levels of Machiavellianism and then train a supervised model to predict the labels directly.</li>
            </ol>
          </div>
      </div>
     </section>

    <footer>
      <p>
        MACH-IV Clustering Project &copy; 2025. Created by Jeffrey Hoelzel Jr, Sean Golez, and Luke Bowen.
      </p>
    </footer>
    </main>
  </body>
</html>