<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="styles/styles.css">
  <title>MACH-IV Clustering</title>
  <body>
    <header>
      <h1>MACH-IV Clustering Exploration</h1>
      <p>Using clustering algorithms to explore patterns in Machiavellianism (MACH-IV) personality data. Created by Jeffrey Hoelzel Jr, Sean Golez, and Luke Bowen.</p>
    </header>

    <nav>
      <ul>
        <li><a href="#dataset">Dataset</a></li>
        <li><a href="#kmeans">K-Means</a></li>
        <li><a href="#gmm">Gaussian Mixture Model</a></li>
        <li><a href="#spectral">Spectral Clustering</a></li>
        <li><a href="#hierarchical">Hierarchical Clustering</a></li>
        <li><a href="#conclusion">Conclusion</a></li>
      </ul>
    </nav>

    <main>
      <section class="intro-card">
        <h2>Project Overview</h2>
        <p>We worked with a sample of Machiavellian response vectors where each participant answered twenty items on a Likert-scale from one to five. Each method grouped people based on their answer patterns, allowing us to compare how different algorithms carve the data into clusters that might correspond to low, medium, and high levels of Machiavellian traits. The dataset used can be found <a target="_blank" href="https://openpsychometrics.org/_rawdata/">here</a>.</p>
        <div class="summary-box">
          <h3>On this site, you can find:</h3>
          <ul>
            <li>Plots that visualize clusters in a low-dimensional space.</li>
            <li>Short written interpretations for each method used.</li>
            <li>Basic validation metrics such as Silhouette scores and PCA.</li>
          </ul>
        </div>
      </section>

    <!-- Dataset -->
     <section id="dataset" class="algorithm-section">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <h2>Dataset</h2>
        </div>
        <div class="algorithm-body-text-only">
          <div class="text-block">
            <p>
              The MACH-IV dataset consists of responses from <strong>73,489 participants</strong> who completed the MACH-IV personality inventory. Each participant answered <strong>20 items on a Likert scale</strong> ranging from 1 (strongly disagree) to 5 (strongly agree). The dataset captures various dimensions of Machiavellian traits, which are often associated with manipulativeness, cynicism, and a pragmatic approach to interpersonal relationships.
            </p>
            <p>
              Prior to applying clustering algorithms, we performed <strong>standard preprocessing</strong> steps including handling missing values and conducting exploratory data analysis to understand the distribution of responses across items. This ensured that our clustering results would be based on clean and meaningful data.
            </p>
            <p>
              There is also much more information provided in this datasets beyond the 20 questions we focused on for this project. When downloading the dataset, there will also be a <strong>codebook provided</strong> that describes all the additional demographic and psychometric variables collected from participants. You can find the raw dataset <a target="_blank" href="https://openpsychometrics.org/_rawdata/">here</a> under the description <i>"Answers to the Machivallianism Test, a version of the MACH-IV from Christie and Geis (1970)."</i>
            </p>
          </div>
          <div class="dataset-sample-table">
            <h3>Sample MACH-IV Responses Used for Clustering</h3>
            <p>Below is a sample of four particpants' responses to the 20 MACH-IV items, scored on a 1-5 likert scale.</p>
            <table class="metrics-table">
              <thead>
                <tr>
                  <th>Participant</th>
                  <th>Q1A</th>
                  <th>Q2A</th>
                  <th>Q3A</th>
                  <th>Q4A</th>
                  <th>Q5A</th>
                  <th>Q6A</th>
                  <th>Q7A</th>
                  <th>Q8A</th>
                  <th>Q9A</th>
                  <th>Q10A</th>
                  <th>Q11A</th>
                  <th>Q12A</th>
                  <th>Q13A</th>
                  <th>Q14A</th>
                  <th>Q15A</th>
                  <th>Q16A</th>
                  <th>Q17A</th>
                  <th>Q18A</th>
                  <th>Q19A</th>
                  <th>Q20A</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>1</td>
                  <td>3</td>
                  <td>3</td>
                  <td>5</td>
                  <td>2</td>
                  <td>3</td>
                  <td>1</td>
                  <td>2</td>
                  <td>4</td>
                  <td>3</td>
                  <td>3</td>
                  <td>1</td>
                  <td>5</td>
                  <td>4</td>
                  <td>1</td>
                  <td>5</td>
                  <td>2</td>
                  <td>4</td>
                  <td>4</td>
                  <td>4</td>
                  <td>4</td>
                </tr>
                <tr>
                  <td>2</td>
                  <td>5</td>
                  <td>5</td>
                  <td>1</td>
                  <td>3</td>
                  <td>5</td>
                  <td>1</td>
                  <td>1</td>
                  <td>4</td>
                  <td>2</td>
                  <td>1</td>
                  <td>1</td>
                  <td>5</td>
                  <td>5</td>
                  <td>2</td>
                  <td>5</td>
                  <td>1</td>
                  <td>2</td>
                  <td>4</td>
                  <td>5</td>
                  <td>3</td>
                </tr>
                <tr>
                  <td>3</td>
                  <td>5</td>
                  <td>5</td>
                  <td>1</td>
                  <td>1</td>
                  <td>5</td>
                  <td>1</td>
                  <td>1</td>
                  <td>5</td>
                  <td>1</td>
                  <td>1</td>
                  <td>1</td>
                  <td>5</td>
                  <td>5</td>
                  <td>1</td>
                  <td>5</td>
                  <td>1</td>
                  <td>1</td>
                  <td>5</td>
                  <td>5</td>
                  <td>3</td>
                </tr>
                <tr>
                  <td>4</td>
                  <td>2</td>
                  <td>4</td>
                  <td>2</td>
                  <td>1</td>
                  <td>5</td>
                  <td>1</td>
                  <td>3</td>
                  <td>2</td>
                  <td>4</td>
                  <td>2</td>
                  <td>1</td>
                  <td>5</td>
                  <td>1</td>
                  <td>1</td>
                  <td>4</td>
                  <td>1</td>
                  <td>1</td>
                  <td>5</td>
                  <td>3</td>
                  <td>4</td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>
      </div>
     </section>

    <!-- K Means -->
    <section id="kmeans" class="algorithm-section">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <h2>K-Means Clustering</h2>
          <span class="algorithm-tag">Partition based</span>
        </div>
        <div class="algorithm-body">
          <div class="text-block">
            <p>
              K-Means partitions the data into a set number of clusters by assigning each participant to the nearest cluster center in feature space. In our analysis, we chose a value of <i>k</i> that balanced interpretability with inter-cluster compactness.
            </p>
            <ul>
              <li>We chose <b>k=2</b> using the <b>Elbow Method</b>, which consists of plotting the explained variation as a function of the number of clusters and picking the elbow of the curve as the number of clusters to use. We also compare <b>silhouette scores</b> at different numbers of clusters.</li>
              <li>Given that K-means assigns two "hard" clusters, this shows that it could best identify two main regions in the data, being answers that reflect a <b>low-level of Machiavellianism</b> and those reflecting a <b>high-level of Machiavellianism</b>.</li>
              <li>Compared to other methods, K-means expects more <b>spherical clusters</b> and assigns <b>clear-cut categories</b>.</li>
            </ul>
            <table class="metrics-table">
              <thead>
                <tr>
                  <th>Metric</th>
                  <th>Value</th>
                  <th>Notes</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Silhouette score</td>
                  <td>0.207</td>
                  <td>Spectral and K-Means both used the same metric of silhouette score for direct comparison.</td>
                </tr>
                <tr>
                  <td>Number of clusters</td>
                  <td>2</td>
                  <td>We evaluated values of K ranging from 2 to 4.</td>
                </tr>
                <tr>
                  <td>Within-Cluster Sum of Squares</td>
                  <td>150828</td>
                  <td>Plotted against increasing k values to find the point where the improvement slows down.</td>
                </tr>
              </tbody>
            </table>
          </div>
          <div class="plot-block">
            <figure>
              <div class="plot-placeholder">
                <img src="images/kmeans/elbow_method.png" alt="Plot showing WCSS for different numbers of clusters">
              </div>
              <figcaption>
                Elbow method plot of WCSS (within-cluster sum of squares) at different numbers of clusters for n=5000 random participants. It can be observed that the rate of decrease in WCSS sharply slows down at k=2.
              </figcaption>
            </figure>
            <figure>
              <div class="plot-placeholder">
                <img src="images/kmeans/kmeans_pca.png" alt="PCA plots for different K-means clusters">
              </div>
              <figcaption>
                PCA plot for 2, 3, & 4 clusters showing K-means clustering results using Ward linkage on all 73,489 participants. The Silhouette scores for 2, 3, and 4 clusters are 0.207, 0.124, and 0.125 respectively.
              </figcaption>
            </figure>
          </div>
        </div>
        <p>
          <br>Below are three radar plots representing the <strong>Ten Item Personality Inventory (TIPI) personality responses</strong> for each of the two clusters identified using K-means clustering. Each plot illustrates the <strong>distinct personality response characteristics of each cluster</strong>, providing insights into the varying levels of Machiavellian traits among participants.<br><br>
        </p>
        <p>
          An important item to consider is the line between low and high Machiavellianism is not very clear according to our research. It it clear that a more in-depth analysis would be required to fully understand the nuances of Machiavellian traits across individuals. However, these plots provide a foundational understanding of how different personality traits cluster together in relation to Machiavellianism.<br><br>
        </p>
        <div class="plot-block-horizontal">
          <figure>
            <div class="plot-placeholder">
              <img src="images/kmeans/Cluster_1.png" alt="Radar plot showing average item responses for cluster 1 using K-means clustering">
            </div>
            <figcaption>
              Cluster 1 represents <strong>low-Machiavellian</strong> individuals. This groups seems to be more outgoing, sympathetic, and organized.
          </figure>
          <figure>
            <div class="plot-placeholder">
              <img src="images/kmeans/Cluster_0.png" alt="Radar plot showing average item responses for cluster 0 using K-means clustering">
            </div>
            <figcaption>
              Cluster 0 reflects <strong>high-Machiavellian</strong> individuals. This groups seems to be less outgoing, sympathetic, and organized.
          </figure>
        </div>
        <div class="architecture-block">
          <h3>K-Means Architecture</h3>
          <div class="plot-placeholder architecture-image">
            <img src="images/kmeans/kmeans_architecture.drawio.png" alt="Architecture diagram for K-Means algorithm">
          </div>
          <figcaption>
            End-to-end architecture for our K-Means workflow, showing how the MACH-IV dataset flows through preprocessing, clustering, labeling & scoring, and final cluster analysis.
          </figcaption>
        </div>
      </div>
    </section>

    <!-- Gaussian mixture model -->
    <section id="gmm" class="algorithm-section">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <h2>Gaussian Mixture Model</h2>
          <span class="algorithm-tag">Model based</span>
        </div>
        <div class="algorithm-body">
          <div class="text-block">
            <p>
              A Gaussian mixture model treats clusters as overlapping probability distributions rather than hard partitions. Each participant receives a probability of belonging to each component, which can reveal more gradual changes between low and high trait levels.
            </p>
            <ul>
              <li>The number of components was chosen using <b>BIC</b>, <b>AIC</b>, and <b>log-likelihood</b>.</li>
              <li>At k=6 we observed see a <b>global minimum</b> of BIC and AIC, and <b>global maximum</b> log-likelihood.</li>
              <li>Based on the PCA plots, GMM seems to be finding clusters as a <b>gradient across the data</b>. Whereas K-means finds distinct sections of the data to cluster.</li>
            </ul>
            <table class="metrics-table">
              <thead>
                <tr>
                  <th>Metric</th>
                  <th>Value</th>
                  <th>Notes</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>BIC</td>
                  <td>146597</td>
                  <td>Favors simpler models more heavily. Lower BIC = better model.</td>
                </tr>
                <tr>
                  <td>AIC</td>
                  <td>137571</td>
                  <td>Balances model fit and complexity using a mild penalty. Lower AIC = better model.</td>
                </tr>
                <tr>
                  <td>Log-likelihood</td>
                  <td>-13.48</td>
                  <td>Measures how well the model explains the data. Higher log-likelihood = better fit.</td>
                </tr>
              </tbody>
            </table>
          </div>
          <div class="plot-block">
            <figure>
              <div class="plot-placeholder">
                <img src="images/gmm/metrics.png" alt="Plot showing BIC, AIC, and log-likelihood for different numbers of clusters">
              </div>
              <figcaption>
                Finding number of clusters with normalized BIC, AIC, and log-likelihood for n=5000 random participants.
              </figcaption>
            </figure>
            <figure>
              <div class="plot-placeholder">
                <img src="images/gmm/gmm_pca.png" alt="PCA plots for different GMM clusters">
              </div>
              <figcaption>
                PCA plot for 2, 4, & 6 clusters showing GMM clustering results using Ward linkage on all 73,489 participants. The Silhouette scores for 2, 4, and 6 clusters are 0.05, 0.03, and -0.04 respectively.
              </figcaption>
            </figure>
          </div>
        </div>
        <p>
          <br>Below are three radar plots representing the <strong>Ten Item Personality Inventory (TIPI) personality responses</strong> for each of the two clusters identified using GMM clustering. Each plot illustrates the <strong>distinct personality response characteristics of each cluster</strong>, providing insights into the varying levels of Machiavellian traits among participants.<br><br>
        </p>
        <p>
          An important item to consider is the line between low and high Machiavellianism is not very clear according to our research. It it clear that a more in-depth analysis would be required to fully understand the nuances of Machiavellian traits across individuals. However, these plots provide a foundational understanding of how different personality traits cluster together in relation to Machiavellianism.<br><br>
        </p>
        <p>
          The middle clusterings seems to be very varied. But, the extreme low and high Machiavellian clusters show a trend the higher Machiavellian group being more anxious, introverted, reserved, and unsympathetic.<br><br>
        </p>
        <div class="plot-block-horizontal">
          <figure>
            <div class="plot-placeholder">
              <img src="images/gmm/Cluster_2.png" alt="Radar plot showing average item responses for cluster using GMM clustering">
            </div>
            <figcaption>
              Cluster represents <strong>low-Machiavellian</strong> individuals.
          </figure>
          <figure>
            <div class="plot-placeholder">
              <img src="images/gmm/Cluster_4.png" alt="Radar plot showing average item responses for cluster using GMM clustering">
            </div>
            <figcaption>
              Cluster represents <strong>lower-Machiavellian</strong> individuals.
          </figure>
          <figure>
            <div class="plot-placeholder">
              <img src="images/gmm/Cluster_3.png" alt="Radar plot showing average item responses for cluster using GMM clustering">
            </div>
            <figcaption>
              Cluster represents <strong>lower-middle-Machiavellian</strong> individuals.
          </figure>
          <figure>
            <div class="plot-placeholder">
              <img src="images/gmm/Cluster_5.png" alt="Radar plot showing average item responses for cluster using GMM clustering">
            </div>
            <figcaption>
              Cluster represents <strong>higher-middle-Machiavellian</strong> individuals.
          </figure>
          <figure>
            <div class="plot-placeholder">
              <img src="images/gmm/Cluster_1.png" alt="Radar plot showing average item responses for cluster using GMM clustering">
            </div>
            <figcaption>
              Cluster represents <strong>higher-Machiavellian</strong> individuals.
          </figure>
          <figure>
            <div class="plot-placeholder">
              <img src="images/gmm/Cluster_0.png" alt="Radar plot showing average item responses for cluster 0 using GMM clustering">
            </div>
            <figcaption>
              Cluster reflects <strong>high-Machiavellian</strong> individuals.
          </figure>
        </div>
        <div class="architecture-block">
          <h3>Gaussian Mixture Model Architecture</h3>
          <div class="plot-placeholder architecture-image">
            <img src="images/gmm/gmm_architecture.drawio.png" alt="Architecture diagram for GMM algorithm">
          </div>
          <figcaption>
            End-to-end architecture for our GMM workflow, showing how the MACH-IV dataset flows through preprocessing, clustering, labeling & scoring, and final cluster analysis.
          </figcaption>
        </div>
      </div>
    </section>

    <!-- Spectral clustering -->
    <section id="spectral" class="algorithm-section">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <h2>Spectral Clustering</h2>
          <span class="algorithm-tag">Graph based</span>
        </div>
        <div class="algorithm-body">
          <div class="text-block">
            <p>
              Spectral clustering builds a <strong>similarity graph</strong> between participants and uses <strong>eigenvectors</strong> of the graph Laplacian to embed the data before forming clusters. This method can capture <strong>more complex structures</strong> that simple distance based partitions might miss.
            </p>
            <ul>
              <li>We constructed a <strong>similarity matrix</strong> by building a k-nearest-neighbor graph
              using Euclidean distances with <i>k=5</i></li>. We then symmetrized the graph using an <strong>RBF kernel</strong>.
              <li>The results from spectral clustering differ from algorithms like kmeans or GMM in the sense that
                spectral clustering can capture <strong>non-convex cluster shapes</strong> and is sensitive to the choice of graph construction method.
                In practice, the clusters identified by spectral clustering showed boundaries that were <strong>smoother</strong> and <strong>better aligned</strong>
                with the underlying data manifold.
              </li>
              <li>Due to the reliance on the graph Laplacian, spectral clustering can be much more sensitive to local-connectivity than KMeans.
                This tends to force points into similarly sized partitions, which we saw when running spectral clustering with more than 2 clusters.
              </li>
            </ul>
            <table class="metrics-table">
              <thead>
                <tr>
                  <th>Metric</th>
                  <th>Value</th>
                  <th>Notes</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Silhouette score</td>
                  <td>0.XX</td>
                  <td>Spectral and KMeans both used the same metric of silhouette score for direct comparison.</td>
                </tr>
                <tr>
                  <td>Number of clusters</td>
                  <td>K</td>
                  <td>We evaluated values of K ranging from 2 to 4.</td>
                </tr>
                <tr>
                  <td>Neighbor parameter</td>
                  <td>k</td>
                  <td>We ran this simulation using k=15 for the k-nearest-neighbors hyperparameter.</td>
                </tr>
              </tbody>
            </table>
          </div>
          <div class="plot-block">
            <figure>
              <div class="plot-placeholder">
                <img src="images/spectral/spectral_embedding_k_2.png" alt="Spectral embedding plot showing clusters identified by spectral clustering">
              </div>
              <figcaption>
                Spectral Embedding plot ran with 2 clusters on Monsoon.
              </figcaption>
            </figure>
            <figure>
              <div class="plot-placeholder">
                <img src="images/spectral/spectral_embedding_k_3.png" alt="Spectral embedding plot showing clusters identified by spectral clustering">
              </div>
              <figcaption>
                Spectral Embedding plot ran with 3 clusters on Monsoon. Note the inability to smoothly form the third cluster due to the local connectivity constraints of spectral clustering.
              </figcaption>
            </figure>
          </div>
        </div>
        <p>
          <br>Below are two radar plots showing the <strong>Ten Item Personality Inventory (TIPI) personality responses</strong> for both clusters identified using spectral clustering. Each plot gives further insight into the clusters discovered, and shows the distinct levels of <strong>Machiavellianism</strong> within each group.<br><br>
        </p>
          <div class="plot-block-horizontal">
            <figure>
              <div class="plot-placeholder">
                <img src="images/spectral/Cluster_1.png" alt="Degree distribution plot of the similarity graph used in spectral clustering">
              </div>
              <figcaption>
                Cluster 1 represents low-Machiavellian individuals. These individuals scored higher in traits like emotionally stable, dependable, and disciplined, while scoring lower in traits like careless, uncreative, or easily upset.
              </figcaption>
            </figure>
            <figure>
              <div class="plot-placeholder">
                <img src="images/spectral/Cluster_0.png" alt="Degree distribution plot of the similarity graph used in spectral clustering">
              </div>
              <figcaption>
                Cluster 0 represents high-Machiavellian individuals. These individuals scored higher in traits like extraverted, anxious, and disorganized, while scoring lower in traits like reserved, calm, and sympathetic.
              </figcaption>
            </figure>
          </div>
        </div>
        <div class="architecture-block">
          <h3>Spectral Clustering Architecture</h3>
          <div class="plot-placeholder architecture-image">
            <img src="images/spectral/spectral_architecture.drawio.png" alt="Architecture diagram for spectral clustering algorithm">
          </div>
          <figcaption>
            End-to-end architecture for our spectral clustering workflow, showing how the MACH-IV dataset flows through preprocessing, labeling & scoring, picking k, plotting spectral embeddings, and final cluster analysis.
          </figcaption>
        </div>
      </div>
    </section>

    <!-- Hierarchical clustering -->
    <section id="hierarchical" class="algorithm-section">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <h2>Hierarchical Clustering</h2>
          <span class="algorithm-tag">Tree based</span>
        </div>
        <div class="algorithm-body">
          <div class="text-block">
            <p>
              Hierarchical clustering builds a tree of merges that shows how clusters form at different distance thresholds. This offers a view of structure, known as a <strong>dendrogram</strong>, at many levels rather than a single partition of the data.
            </p>
            <p>
              In our final analysis, we ran Ward linkage hierarchical clustering on <strong>all 73,489 particpants</strong> using NAU's <strong>Monsoon HPC</strong> to handle the computational load. We then sampled <strong>5,000 random participants</strong> to visualize the dendrograms for each linkage rule.
            </p>
            <ul>
              <li>Our final hierarchical clustering algorithm uses <strong>agglomerative clustering</strong> with the <strong>Ward</strong> linkage rule.</li>
              <li>To choose a cut level, we plotted the dendrogram <i>(as seen on the right)</i> and determined that <strong>2, 3, & 4</strong> clusters were good cluster sizes to try.</li>
              <li>We determined <strong>3 clusters</strong> was the sweet spot for hierarchical clustering based on the <strong>PCA plot</strong> <i>(bottom right)</i>, which aligns with most personality-based studies suggesting that a <strong>spectrum</strong> exists among general personality traits.</li>
              <li>Although, each of the three Silhouette scores are <i>fairly low</i>, suggesting there are not clear clusters among our dataset, we expected this since the data is related to personality-based data which is often <strong>more of a spectrum</strong> rather than clear-cut categories.</li>
            </ul>
            <table class="metrics-table">
              <thead>
                <tr>
                  <th>Feature</th>
                  <th>Choice</th>
                  <th>Notes</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Distance metric</td>
                  <td>Euclidean</td>
                  <td>Ward linkage tries to minimize the increase in total inter-cluster variation by minimizing the sum of squared Euclidean distances.</td>
                </tr>
                <tr>
                  <td>Linkage rule</td>
                  <td>Ward</td>
                  <td>Ward linkage offers the most interpretable hierarchy for psychological traits and thus, became our linkage rule of choice.</td>
                </tr>
                <tr>
                  <td>Number of clusters at cut</td>
                  <td>3</td>
                  <td>We decided 3 clusters made the most sense to interpret this data.</td>
                </tr>
              </tbody>
            </table>
          </div>
          <div class="plot-block">
            <figure>
              <div class="plot-placeholder">
                <img src="images/hierarchical/default_dendrograms.png" alt="Four dendrograms showing hierarchical clustering using the four different linkage rules for n=5000 random participants">
              </div>
              <figcaption>
                Four dendrograms showing hierarchical clustering using the four different linkage rules for n=5000 random participants.
              </figcaption>
            </figure>
            <figure>
              <div class="plot-placeholder plot-wide">
                <img src="images/hierarchical/ward_linkage_pca.png" alt="PCA plot showing hierarchical clustering results using Ward linkage on all 73,489 participants">
              </div>
              <figcaption>
                PCA plot for 2, 3, & 4 clusters showing hierarchical clustering results using Ward linkage on all 73,489 participants. The Silhouette scores for 2, 3, and 4 clusters are 0.174, 0.108, and 0.049 respectively.
              </figcaption>
            </figure>
          </div>
        </div>
        <p>
          <br>Below are three radar plots representing the <strong>Ten Item Personality Inventory (TIPI) personality responses</strong> for each of the three clusters identified using hierarchical clustering with Ward linkage. Each plot illustrates the <strong>distinct personality response characteristics of each cluster</strong>, providing insights into the varying levels of Machiavellian traits among participants.<br><br>
        </p>
        <p>
          An important item to consider is the line between low and high Machiavellianism is not very clear according to our research. It it clear that a more in-depth analysis would be required to fully understand the nuances of Machiavellian traits across individuals. However, these plots provide a foundational understanding of how different personality traits cluster together in relation to Machiavellianism.<br><br>
        </p>
        <div class="plot-block-horizontal">
          <figure>
            <div class="plot-placeholder">
              <img src="images/hierarchical/Cluster_2.png" alt="Radar plot showing average item responses for cluster 2 using hierarchical clustering with Ward linkage">
            </div>
            <figcaption>
              Cluster 2 represents <strong>low-Machiavellian</strong> individuals who score high in conscientiousness and emotional stability while remaining warm, calm, and cooperative. Their profile reflects a trusting and disciplined personality with little inclination toward manipulation. This group tends to value honesty, structure, and positive social relationships.
          </figure>
          <figure>
            <div class="plot-placeholder">
              <img src="images/hierarchical/Cluster_3.png" alt="Radar plot showing average item responses for cluster 3 using hierarchical clustering with Ward linkage">
            </div>
            <figcaption>
              Cluster 3 shows a moderate, balanced personality profile with no extreme traits in any direction. These individuals fall in the <strong>middle of the Machiavellian</strong> spectrum and demonstrate flexible, context-dependent behavior. They are neither strongly manipulative nor strongly moralistic.
          </figure>
          <figure>
            <div class="plot-placeholder">
              <img src="images/hierarchical/Cluster_1.png" alt="Radar plot showing average item responses for cluster 1 using hierarchical clustering with Ward linkage">
            </div>
            <figcaption>
              Cluster 1 reflects <strong>high-Machiavellian</strong> tendencies characterized by low warmth, high criticalness, strong emotional control, and strategic thinking. These individuals display traits associated with competitiveness, manipulation, and a pragmatic worldview. Their combination of low agreeableness and high conscientiousness aligns with classic Mach personality patterns.
          </figure>
        </div>
        <div class="architecture-block">
          <h3>Hierarchical Clustering Architecture</h3>
          <div class="plot-placeholder architecture-image">
            <img src="images/hierarchical/hierarchical_architecture.drawio.png" alt="Architecture diagram for hierarchical clustering algorithm">
          </div>
          <figcaption>
            End-to-end architecture for our hierarchical clustering workflow, showing how the MACH-IV dataset flows through preprocessing, Ward linkage computation, dendrogram generation, scoring, and final cluster analysis.
          </figcaption>
        </div>
      </div>
    </section>

    <!-- Conclusion -->
    <section id="conclusion" class="algorithm-section">
      <div class="algorithm-card">
        <div class="algorithm-header">
          <h2>Conclusion</h2>
        </div>
        <div class="algorithm-body-text-only">
          <div class="text-block">
            <p>
              In this project we applied several clustering algorithms, including K-Means, Gaussian mixture models, spectral clustering, and hierarchical clustering with Ward linkage, to a large sample of MACH-IV response vectors. Across methods we consistently observed that clear, compact clusters are difficult to obtain and that Machiavellianism in this dataset behaves more like a spectrum rather than a set of discrete types, which is reflected in the relatively low silhouette scores. Even so, the resulting clusters reveal meaningful contrasts between lower and higher Machiavellian profiles and highlight how related personality traits co-vary across groups. Capturing more nuanced structure in this space will likely require more expressive nonlinear models, such as neural networks or autoencoder based representations, that can learn richer embeddings beyond the raw Likert-scale vectors.
            </p>
          </div>
        </div>
      </div>
    </section>

    <footer>
      <p>
        MACH-IV Clustering Project &copy; 2025. Created by Jeffrey Hoelzel Jr, Sean Golez, and Luke Bowen.
      </p>
    </footer>
    </main>
  </body>
</html>